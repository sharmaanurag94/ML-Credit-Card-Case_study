{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><u>Machine Learning</u></h2>\n",
    "\n",
    "# Case Study - Credit Card Payment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case study, you will apply various classification algorithms to predict the payment for credit card for next month is defaulted or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Load Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in /opt/conda/lib/python3.7/site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in /opt/conda/lib/python3.7/site-packages (from imblearn) (0.6.1)\n",
      "Requirement already satisfied: numpy>=1.11 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (1.17.0)\n",
      "Requirement already satisfied: scipy>=0.17 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (1.2.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (0.13.2)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /opt/conda/lib/python3.7/site-packages (from imbalanced-learn->imblearn) (0.22.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings # to ignore warning\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Load and analyse data\n",
    "\n",
    "        - Load the data from the required location into a DataFrame\n",
    "        - Analyse the shape of the data by printing its total number of rows & columns\n",
    "        - Also print 5 rows of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit = pd.read_csv(\"UCI_Credit_Card.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit.head() ### Glimpse of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Clean the data\n",
    "\n",
    "\n",
    "    - ID variable as it has no relevance to training a model\n",
    "    - Check for any null values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing ID variable as it has no relevnce for logistic regression model\n",
    "credit.drop(['ID'],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for any null values\n",
    "credit.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no null values in the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Check Description \n",
    "       \n",
    "       - 7 point statistics for the continous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - We know that many of the variable are categorical variabble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Check either the data is balanced or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking for data unbalance\n",
    "temp = credit[\"default.payment.next.month\"].value_counts()\n",
    "df = pd.DataFrame({'default.payment.next.month': temp.index,'values': temp.values})\n",
    "plt.figure(figsize = (6,6))\n",
    "plt.title('Default Credit Card Clients - target value - data unbalance\\n (Default = 0, Not Default = 1)')\n",
    "sns.set_color_codes(\"pastel\")\n",
    "sns.barplot(x = 'default.payment.next.month', y=\"values\", data=df)\n",
    "locs, labels = plt.xticks()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around 22% of clients will default next month. The data has not a large unbalance with respect of the target value (default.payment.next.month)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Correlation for only numeric variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = ['BILL_AMT1','BILL_AMT2','BILL_AMT3','BILL_AMT4','BILL_AMT5','BILL_AMT6']\n",
    "\n",
    "plt.figure(figsize = (8,8))\n",
    "plt.title('Amount of bill statement (Apr-Sept) \\ncorrelation plot (Pearson)')\n",
    "corr = credit[var].corr()\n",
    "sns.heatmap(corr,xticklabels=corr.columns,yticklabels=corr.columns)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation seems fine for the variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Treatment of categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treating the categorical features to introduce into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = ['EDUCATION', 'SEX', 'MARRIAGE', 'PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_dummies = pd.get_dummies(credit, columns = cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Default of Credit Card Clients train data -  rows:\",credit_dummies.shape[0],\" columns:\", credit_dummies.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target= 'default.payment.next.month'\n",
    "predictors = ['LIMIT_BAL', 'AGE', 'BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4',\n",
    "       'BILL_AMT5', 'PAY_AMT1', 'PAY_AMT2', 'PAY_AMT3',\n",
    "       'PAY_AMT4', 'PAY_AMT5',\n",
    "       'EDUCATION_0', 'EDUCATION_1', 'EDUCATION_2', 'EDUCATION_3',\n",
    "       'EDUCATION_4', 'EDUCATION_5', 'SEX_1',\n",
    "       'MARRIAGE_0', 'MARRIAGE_1', 'MARRIAGE_2', 'PAY_0_-2',\n",
    "       'PAY_0_-1', 'PAY_0_0', 'PAY_0_1', 'PAY_0_2', 'PAY_0_3', 'PAY_0_4',\n",
    "       'PAY_0_5', 'PAY_0_6', 'PAY_0_7', 'PAY_2_-2', 'PAY_2_-1',\n",
    "       'PAY_2_0', 'PAY_2_1', 'PAY_2_2', 'PAY_2_3', 'PAY_2_4', 'PAY_2_5',\n",
    "       'PAY_2_6', 'PAY_2_7', 'PAY_3_-2', 'PAY_3_-1', 'PAY_3_0',\n",
    "       'PAY_3_1', 'PAY_3_2', 'PAY_3_3', 'PAY_3_4', 'PAY_3_5', 'PAY_3_6',\n",
    "       'PAY_3_7', 'PAY_4_-2', 'PAY_4_-1', 'PAY_4_0', 'PAY_4_1',\n",
    "       'PAY_4_2', 'PAY_4_3', 'PAY_4_4', 'PAY_4_5', 'PAY_4_6', 'PAY_4_7',\n",
    "       'PAY_5_-2', 'PAY_5_-1', 'PAY_5_0', 'PAY_5_2', 'PAY_5_3',\n",
    "       'PAY_5_4', 'PAY_5_5', 'PAY_5_6', 'PAY_5_7', 'PAY_6_-2',\n",
    "       'PAY_6_-1', 'PAY_6_0', 'PAY_6_2', 'PAY_6_3', 'PAY_6_4', 'PAY_6_5',\n",
    "       'PAY_6_6', 'PAY_6_7']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Divide target and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigning and dividing the dataset\n",
    "X = credit_dummies[predictors]\n",
    "y=credit_dummies['default.payment.next.month']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Create training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection  import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Apply Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(C=0.4,max_iter=1000,solver='liblinear')\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Predicting on X_test dataset\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Assessing Model performance\n",
    "    - __Precision__: Percentage of correct results\n",
    "    - __Recall__: Percentage of valid results correctly classified\n",
    "    - __F1 Score__: A measure of test's accuracy which is harmonic mean of precision and recall. Maximising this improves the model. Perfect at 1 and worst at 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "results = pd.DataFrame([['Logistic Regression', acc, prec, rec, f1]],\n",
    "               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Apply SMOTE as data is unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = SMOTE(random_state=589)\n",
    "X_SMOTE, y_SMOTE = sm.fit_sample(X_train, y_train)\n",
    "print(len(y_SMOTE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier1 = LogisticRegression(C=0.4,max_iter=1000,solver='liblinear')\n",
    "classifier1.fit(X_SMOTE, y_SMOTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Predicting on X_test dataset\n",
    "y_pred1 = classifier1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_test, y_pred1)\n",
    "prec = precision_score(y_test, y_pred1)\n",
    "rec = recall_score(y_test, y_pred1)\n",
    "f1 = f1_score(y_test, y_pred1)\n",
    "\n",
    "model_results = pd.DataFrame([['Logistic Regression - with SMOTE', acc, prec, rec, f1]],\n",
    "               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "\n",
    "results = results.append(model_results, ignore_index = True)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test,y_pred1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy has dipped but F1 score of the model has improved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Apply Decison Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state=14) \n",
    "# training the classifier\n",
    "clf.fit(X_train, y_train)\n",
    "# do our predictions on the test\n",
    "pred_dt = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting Test Set\n",
    "acc = accuracy_score(y_test, pred_dt)\n",
    "prec = precision_score(y_test, pred_dt)\n",
    "rec = recall_score(y_test, pred_dt)\n",
    "f1 = f1_score(y_test, pred_dt)\n",
    "\n",
    "model_results = pd.DataFrame([['Decision Tree', acc, prec, rec, f1]],\n",
    "               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "\n",
    "results = results.append(model_results, ignore_index = True)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Apply Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf_rf = RandomForestClassifier()\n",
    "clf_rf.fit(X_train, y_train)\n",
    "\n",
    "# Predicting Test Set\n",
    "y_pred_rf = clf_rf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred_rf)\n",
    "prec = precision_score(y_test, y_pred_rf)\n",
    "rec = recall_score(y_test, y_pred_rf)\n",
    "f1 = f1_score(y_test, y_pred_rf)\n",
    "\n",
    "model_results = pd.DataFrame([['Random Forest(Gini)', acc, prec, rec, f1]],\n",
    "               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "\n",
    "results = results.append(model_results, ignore_index = True)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf_rf = RandomForestClassifier(criterion='entropy')\n",
    "clf_rf.fit(X_train, y_train)\n",
    "\n",
    "# Predicting Test Set\n",
    "y_pred_rf = clf_rf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred_rf)\n",
    "prec = precision_score(y_test, y_pred_rf)\n",
    "rec = recall_score(y_test, y_pred_rf)\n",
    "f1 = f1_score(y_test, y_pred_rf)\n",
    "\n",
    "model_results = pd.DataFrame([['Random Forest(Entropy)', acc, prec, rec, f1]],\n",
    "               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "\n",
    "results = results.append(model_results, ignore_index = True)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Top 5 Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result = pd.DataFrame({'features':X_train.columns,'score':clf.feature_importances_.tolist()})\n",
    "result.sort_values(by=['score'],ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_rf = pd.DataFrame({'features':X_train.columns,'score':clf_rf.feature_importances_.tolist()})\n",
    "result_rf.sort_values(by=['score'],ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference in feature importance provided by random forest classfier vs Decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Apply SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svm = SVC(cache_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svm.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model takes lot of time to run.  Hence caution before you run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting Test Set\n",
    "predicted= model_svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = accuracy_score(y_test, predicted)\n",
    "prec = precision_score(y_test, predicted,zero_division=True)\n",
    "rec = recall_score(y_test, predicted,zero_division=True)\n",
    "f1 = f1_score(y_test, predicted,zero_division=True)\n",
    "\n",
    "model_results = pd.DataFrame([['Support Vector Machine', acc, prec, rec, f1]],\n",
    "               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "\n",
    "results = results.append(model_results, ignore_index = True)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Apply KNN with N = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_knn3 = KNeighborsClassifier(n_neighbors=3)\n",
    "model_knn3.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Apply KNN with N = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_knn4 = KNeighborsClassifier(n_neighbors=4)\n",
    "model_knn4.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Apply KNN with N = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_knn5 = KNeighborsClassifier(n_neighbors=5)\n",
    "model_knn5.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting Test Set N=3\n",
    "pred_knn3= model_knn3.predict(X_test)\n",
    "acc = accuracy_score(y_test, pred_knn3)\n",
    "prec = precision_score(y_test, pred_knn3)\n",
    "rec = recall_score(y_test, pred_knn3)\n",
    "f1 = f1_score(y_test, pred_knn3)\n",
    "\n",
    "model_results = pd.DataFrame([['KNN-3 neigbours', acc, prec, rec, f1]],\n",
    "               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "\n",
    "results = results.append(model_results, ignore_index = True)\n",
    "\n",
    "\n",
    "# Predicting Test Set N=4\n",
    "pred_knn4= model_knn4.predict(X_test)\n",
    "acc = accuracy_score(y_test, pred_knn4)\n",
    "prec = precision_score(y_test, pred_knn4)\n",
    "rec = recall_score(y_test, pred_knn4)\n",
    "f1 = f1_score(y_test, pred_knn4)\n",
    "\n",
    "model_results = pd.DataFrame([['KNN-4 neigbours', acc, prec, rec, f1]],\n",
    "               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "\n",
    "results = results.append(model_results, ignore_index = True)\n",
    "\n",
    "\n",
    "# Predicting Test Set N=5\n",
    "pred_knn5= model_knn5.predict(X_test)\n",
    "acc = accuracy_score(y_test, pred_knn5)\n",
    "prec = precision_score(y_test, pred_knn5)\n",
    "rec = recall_score(y_test, pred_knn5)\n",
    "f1 = f1_score(y_test, pred_knn5)\n",
    "\n",
    "model_results = pd.DataFrame([['KNN-5 neigbours', acc, prec, rec, f1]],\n",
    "               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "\n",
    "results = results.append(model_results, ignore_index = True)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Apply Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, it is possible to apply gaussian and bernoulli naive bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Apply Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting Test Set\n",
    "pred_gnb = gnb.predict(X_test)\n",
    "acc = accuracy_score(y_test, pred_gnb)\n",
    "prec = precision_score(y_test, pred_gnb)\n",
    "rec = recall_score(y_test, pred_gnb)\n",
    "f1 = f1_score(y_test, pred_gnb)\n",
    "\n",
    "model_results = pd.DataFrame([['Gaussian Naive Bayes', acc, prec, rec, f1]],\n",
    "               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "\n",
    "results = results.append(model_results, ignore_index = True)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Apply Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bnb = BernoulliNB()\n",
    "model_bnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting Test Set\n",
    "pred_bnb = model_bnb.predict(X_test)\n",
    "acc = accuracy_score(y_test, pred_bnb)\n",
    "prec = precision_score(y_test, pred_bnb)\n",
    "rec = recall_score(y_test, pred_bnb)\n",
    "f1 = f1_score(y_test, pred_bnb)\n",
    "\n",
    "model_results = pd.DataFrame([['Bernoulli Naive Bayes', acc, prec, rec, f1]],\n",
    "               columns = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "\n",
    "results = results.append(model_results, ignore_index = True)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Which model fits the best?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Check Best Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "max_acc_index=results.Accuracy[results.Accuracy==results.Accuracy.max()].index[0]\n",
    "plt.barh(results.Model,results.Accuracy,color='c')\n",
    "plt.barh(results.Model[max_acc_index],results.Accuracy[max_acc_index],color='m')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Check Best Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "max_pre_index=results.Precision[results.Precision==results.Precision.max()].index[0]\n",
    "plt.barh(results.Model,results.Precision,color='c')\n",
    "plt.barh(results.Model[max_pre_index],results.Precision[max_pre_index],color='m')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Check Best Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "max_rc_index=results.Recall[results.Recall==results.Recall.max()].index[0]\n",
    "plt.barh(results.Model,results.Recall,color='c')\n",
    "plt.barh(results.Model[max_rc_index],results.Recall[max_rc_index],color='m')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Best F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "max_f1_index=results['F1 Score'][results['F1 Score']==results['F1 Score'].max()].index[0]\n",
    "plt.barh(results.Model,results['F1 Score'],color='c')\n",
    "plt.barh(results.Model[max_f1_index],results.Accuracy[max_f1_index],color='m')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on accuracy Random Forest(Gini) is the best model. On considering other parameters of the model it is performing well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i>Conclusion</i></b>: In this demonstration of the case study, we have gained an understanding of how to apply various data pre-processing steps, apply SMOTE and different classification algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
